{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e5362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734b2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adffffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_images = len(mnist.data.values)\n",
    "all_images = mnist.data.values\n",
    "all_labels = np.array(list(map(int, mnist.target.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17be4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = all_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1657c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_images, all_labels, test_size=0.3, shuffle=True, random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb8a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0439ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a565a9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3438701b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceea493",
   "metadata": {},
   "source": [
    "# Create and train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7cea2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "766/766 [==============================] - 87s 114ms/step - loss: 0.1398 - sparse_categorical_accuracy: 0.9569\n",
      "Epoch 2/5\n",
      "766/766 [==============================] - 86s 112ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9858\n",
      "Epoch 3/5\n",
      "766/766 [==============================] - 89s 117ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9897\n",
      "Epoch 4/5\n",
      "766/766 [==============================] - 88s 114ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9929\n",
      "Epoch 5/5\n",
      "766/766 [==============================] - 86s 113ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "teacher = create_teacher_model()\n",
    "fit_model_to_data(teacher, X_train, y_train)\n",
    "#evaluate_model(teacher, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874af1f",
   "metadata": {},
   "source": [
    "# Distill knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4665badf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1532/1532 [==============================] - 21s 13ms/step - sparse_categorical_accuracy: 0.8960 - student_loss: 0.4319 - distillation_loss: 2.6823\n",
      "Epoch 2/3\n",
      "1532/1532 [==============================] - 21s 13ms/step - sparse_categorical_accuracy: 0.9637 - student_loss: 0.1550 - distillation_loss: 0.7416\n",
      "Epoch 3/3\n",
      "1532/1532 [==============================] - 20s 13ms/step - sparse_categorical_accuracy: 0.9726 - student_loss: 0.1075 - distillation_loss: 0.4987\n",
      "657/657 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9737130386313384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = create_student_model()\n",
    "\n",
    "student_copy = keras.models.clone_model(student)\n",
    "\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "\n",
    "distiller.compile(\n",
    "    optimizer='adam',\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=3,\n",
    ")\n",
    "distiller.fit(X_train, y_train.astype(int), epochs=3)\n",
    "\n",
    "evaluate_model(student, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8cd54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.4513 - sparse_categorical_accuracy: 0.8676\n",
      "Epoch 2/3\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.1757 - sparse_categorical_accuracy: 0.9466\n",
      "Epoch 3/3\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.1214 - sparse_categorical_accuracy: 0.9629\n",
      "657/657 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9664464411270824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_copy.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "fit_model_to_data(student_copy, X_train, y_train, 3)\n",
    "evaluate_model(student_copy, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4abc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teacher_model():\n",
    "    model = Sequential(name=\"MNIST_Classifier\")\n",
    "    model.add(Input(shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\", activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43653f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_student_model():\n",
    "    model = Sequential(name=\"MNIST_Classifier\")\n",
    "    model.add(Input(shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\", activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "836fb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_to_data(model, x_train, y_train, epochs=5):\n",
    "    model.fit(\n",
    "        x_train, # Samples\n",
    "        y_train, # Labels\n",
    "        batch_size=64,\n",
    "        epochs=epochs\n",
    "        #verbose=0\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    one_hot_predictions = model.predict(X_test)\n",
    "    label_predictions = np.argmax(one_hot_predictions, axis=1)\n",
    "    y_test = y_test.astype(int)\n",
    "    return f1_score(y_test, label_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60396cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
