{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cedba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import scipy.stats\n",
    "from itertools import islice\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = mnist.data.values\n",
    "all_labels = np.array(list(map(int, mnist.target.values)))\n",
    "all_images = all_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fef58",
   "metadata": {},
   "source": [
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_images, all_labels, test_size=0.3, shuffle=True, random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2500b7",
   "metadata": {},
   "source": [
    "## Create data owners with even distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e65834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOwner:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into chunks with the provided distribution\n",
    "distribution = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
    "data_chunks = chunk_data(X_train, y_train, distribution)\n",
    "dataOwners = []\n",
    "\n",
    "# Create dataOwner instances, with data and temporary ML model\n",
    "for d_c in data_chunks:\n",
    "\n",
    "    # Give a data owner its portion of data\n",
    "    new_owner = DataOwner(d_c[0], d_c[1])\n",
    "\n",
    "    # Create a simple model for each data owner \n",
    "    # This will be updated with the weights from global in the future\n",
    "    new_owner.model = create_simple_model()\n",
    "\n",
    "    dataOwners.append(new_owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfa711",
   "metadata": {},
   "source": [
    "## Train data owners individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27b9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_cycles = 0\n",
    "\n",
    "while nr_cycles < 40:\n",
    "    for owner in dataOwners:\n",
    "        fit_model_to_data(owner.model, owner.data, owner.labels, n_classes=10, epochs=2)\n",
    "    nr_cycles += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a7f46",
   "metadata": {},
   "source": [
    "## Federated Learning Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global model\n",
    "global_model = create_simple_model()\n",
    "global_amount_data = len(X_train)\n",
    "nr_cycles = 0\n",
    "while nr_cycles < 40:\n",
    "    # Extract weights from global model\n",
    "    global_weights = global_model.get_weights()\n",
    "    # Simulating sending the global model to the data owners\n",
    "    for owner in dataOwners: \n",
    "        owner_model = owner.model\n",
    "        owner_model.set_weights(global_weights)\n",
    "    # Train data owners on their own data\n",
    "    for owner in dataOwners:\n",
    "        fit_model_to_data(\n",
    "            owner.model, \n",
    "            owner.data, \n",
    "            owner.labels, \n",
    "            n_classes=10, \n",
    "            epochs=2\n",
    "        )\n",
    "    # Simulate the data owners sending their weights to central\n",
    "    # and calculate data owner weight scaling factor\n",
    "    owner_weights = []\n",
    "    scaling_factor_owners = []\n",
    "    \n",
    "    for owner in dataOwners:\n",
    "        weights = np.array(owner.model.get_weights())\n",
    "        scaling_factor = len(owner.data) / global_amount_data\n",
    "        \n",
    "        owner_weights.append(weights)\n",
    "        scaling_factor_owners.append(scaling_factor)\n",
    "        \n",
    "    # Construct new global model from data owner weights\n",
    "    new_global_weights = np.array(global_weights) * 0 # Create empty weights array \n",
    "    for scaling_factor, weights in zip(scaling_factor_owners, owner_weights):\n",
    "        new_global_weights += weights * scaling_factor  \n",
    "\n",
    "    # Set the new weights on the global model\n",
    "    global_model.set_weights(new_global_weights)\n",
    "    acc = evaluate_model(global_model, X_test, y_test)\n",
    "    \n",
    "    nr_cycles += 1\n",
    "    print(\"Cycle \", nr_cycles, \" complete. Global model accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78db9aa",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    model = Sequential(name=\"MNIST_Classifier\")\n",
    "    model.add(Dense(784, input_shape = (784,), activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2143514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_to_data(model, X, y, n_classes, epochs):\n",
    "    y_one_hot = to_categorical(y, n_classes)\n",
    "    model.fit(\n",
    "        X, # Samples\n",
    "        y_one_hot, # Labels\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    one_hot_predictions = model.predict(X_test)\n",
    "    label_predictions = np.argmax(one_hot_predictions, axis=1)\n",
    "    return f1_score(y_test, label_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b02862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_array(array, distribution):\n",
    "    distribution = np.array(distribution) * len(array)\n",
    "    distribution = [int(d) for d in distribution]\n",
    "    \n",
    "    it = iter(array)\n",
    "    return [np.array(list(islice(it, 0, i))) for i in distribution]\n",
    "\n",
    "def chunk_data(data, labels, distribution):\n",
    "    data_chunks = chunk_array(data, distribution)\n",
    "    label_chunks = chunk_array(labels, distribution)\n",
    "    return list(zip(data_chunks, label_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad48596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(dataOwners):\n",
    "    f, axs = plt.subplots(1,2,figsize=(12, 7))\n",
    "    for i in range(len(dataOwners)):\n",
    "        labels = dataOwners[i].labels\n",
    "\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.title('Owner {}'.format(i+1))\n",
    "        plt.ylim([0, 4500])\n",
    "        plt.xlim([-1,10])\n",
    "        plt.ylabel(\" \")\n",
    "        plt.yticks([])\n",
    "        plt.xticks(list(np.arange(0,10)))\n",
    "        sns.histplot(labels, bins=10, discrete=True)\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.1,\n",
    "                    hspace=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
